{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e7686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nName: James Davis\\nDate: May 18, 2022\\nCourse: Ban601\\nAssignment: Module_2\\n\\nTask:\\n1)\\tWhat are the specific anomalies you uncovered in your dataset? \\n    Note: Be sure they are different from the ones Thomas pointed out. \\n    Why are they anomalies? \\n    How did you specifically address them? NOTE: be specific and precise.\\n2)\\tFrom your First Cut Predictive Model, what coefficient estimates have \\n    signs that do not seem to make sense from a theoretical or business perspective? \\n    Explain why those signs do not make sense.\\n3)\\tWhat are both (1) the direction (under-predicting or over-predicting)\\n    and (2) magnitude of the forecast errors from this First Cut Predictive Model?\\n\\nGiven:\\nWest_Roxbury_Data_Set_Only.xlsx\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Name: James Davis\n",
    "Date: May 18, 2022\n",
    "Course: Ban601\n",
    "Assignment: Module_2\n",
    "\n",
    "Task:\n",
    "1)\tWhat are the specific anomalies you uncovered in your dataset? \n",
    "    Note: Be sure they are different from the ones Thomas pointed out. \n",
    "    Why are they anomalies? \n",
    "    How did you specifically address them? NOTE: be specific and precise.\n",
    "2)\tFrom your First Cut Predictive Model, what coefficient estimates have \n",
    "    signs that do not seem to make sense from a theoretical or business perspective? \n",
    "    Explain why those signs do not make sense.\n",
    "3)\tWhat are both (1) the direction (under-predicting or over-predicting)\n",
    "    and (2) magnitude of the forecast errors from this First Cut Predictive Model?\n",
    "\n",
    "Given:\n",
    "West_Roxbury_Data_Set_Only.xlsx\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1ef587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe1a1ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WEST ROXBURY DATA SET ONLY.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-34fdafe684e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load Dataset as train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Note: Load sheet 2 'Data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WEST ROXBURY DATA SET ONLY.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WEST ROXBURY DATA SET ONLY.xlsx'"
     ]
    }
   ],
   "source": [
    "#Load Dataset as train \n",
    "#Note: Load sheet 2 'Data'\n",
    "df_train = pd.read_excel('WEST ROXBURY DATA SET ONLY.xlsx','Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Vars\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ac440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up name values \n",
    "#Noticed a space after some headers\n",
    "df_train.rename(columns = {'TOTAL VALUE ': 'Total_Value'\n",
    "                           ,'TAX': 'Tax_Cost'\n",
    "                           ,'LOT SQFT ': 'Lot_SQFT'\n",
    "                           ,'YR BUILT': 'Year_Built'\n",
    "                           ,'GROSS AREA ': 'Gross_Area'\n",
    "                           ,'LIVING AREA': 'Living_Area'\n",
    "                           ,'FLOORS': 'Floor_Count'\n",
    "                           ,'ROOMS': 'Room_Count'\n",
    "                           ,'BEDROOMS ': 'Bedroom_Count'\n",
    "                           ,'FULL BATH': 'Full_Bath_Count'\n",
    "                           ,'HALF BATH': 'Half_Bath_Count'\n",
    "                           ,'KITCHEN': 'Kitchen_Count'\n",
    "                           ,'FIREPLACE': 'Fireplace_Count'\n",
    "                           ,'REMODEL': 'Remodel'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85509735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe Sale Price \n",
    "print(df_train.describe())\n",
    "\n",
    "print(df_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d545a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any NUlls or zeros?\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "zeros = df_train.isin([0]).sum().sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent,zeros], axis=1, keys=['Total', 'Percent','Zero_Count'])\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c54d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove that zero \"Year_Built\"\n",
    "df_train = df_train[df_train['Year_Built'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(df_train,\n",
    "            y_vars = [\"Total_Value\"],\n",
    "            x_vars = ['Tax_Cost', 'Lot_SQFT', 'Year_Built', 'Gross_Area',\n",
    "       'Living_Area', 'FLOORS ', 'Room_Count', 'Bedroom_Count',\n",
    "       'Full_Bath_Count', 'Half_Bath_Count', 'Kitchen_Count',\n",
    "       'Fireplace_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd709319",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "\n",
    "Tax_Cost is perfectly linear -- #remove\n",
    "Lot_SQFT is compact and not as linear as I wanted it to be\n",
    "Year_Built shows no linear concepts, this was a huge surprise \n",
    "to me, but it makes sense, if I have a family of 4 and everyone needs a room\n",
    "I need 4 rooms, regardless of how large the lot may be.. \n",
    "Gross_Area and Living Area are pretty linear! \n",
    "Room_Count and Bedroom_Count are linear with a few outliers\n",
    "\n",
    "#Check correlation of rooms to Gross_Area I assume more rooms == Greater area of houses\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop tax cost column\n",
    "df_train.drop('Tax_Cost', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find how correlated heat map\n",
    "#saleprice correlation matrix\n",
    "corrmat = df_train.corr()\n",
    "k = 100 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Total_Value')['Total_Value'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "\n",
    "WOW - Year_Built is almost meaningless compared to Total_Value\n",
    "#Check if true \n",
    "\n",
    "Living_Area is more correlated than Gross_Area\n",
    "Rooms are more correlated than bedrooms\n",
    "\n",
    "Rooms, bedrooms are more valuable than lot square footage\n",
    "\n",
    "Lot_SQFT is more correlated than bathrooms.. \n",
    "I guess we can share a \n",
    "bathroom and have more area?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05445c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Year_Built'\n",
    "data = pd.concat([df_train['Total_Value'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"Total_Value\", data=data);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed12973",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "wow.. yeah that is pretty flat\n",
    "Year_Built in this dataset is almost meaningless\n",
    "Houses built in the earlier years \n",
    "have similar price dist compared to younger houses\n",
    "houses built in the middle years are lower\n",
    "I assume more houses were needed, more were built \n",
    "with uniformity and probably the outliers are remodels or\n",
    "have a larger quanity of rooms/ greater living areas\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac8bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for skewness for all columns\n",
    "numeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df_train[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "print(skewness.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0250ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Distribution of Total Value\n",
    "#histogram\n",
    "sns.distplot(df_train['Total_Value'])\n",
    "\n",
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['Total_Value'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['Total_Value'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "Total_Value is highly '1.3' Positive Skewed\n",
    "I think this means Houses are being sold for less than the average,\n",
    "we will need to fix it with logs\n",
    "\n",
    "Kurtosis > 3 so Total_Value is Leptokurtic\n",
    "Total_Value does show peakness with tails longer \n",
    "on the right side.. check this later.. may\n",
    "flatten because of outliers\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dab302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for Normality\n",
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['Total_Value'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Total_Value'], plot=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a97ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "peaked and positive\n",
    "does not follow theo lines in prob plot\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42db349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying log transformation\n",
    "df_train['Total_Value'] = np.log(df_train['Total_Value'])\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['Total_Value'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Total_Value'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['Living_Area'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Living_Area'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "Living area is very skewed, let's fix with logs  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c94262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation\n",
    "df_train['Living_Area'] = np.log(df_train['Living_Area'])\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['Living_Area'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Living_Area'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd815737",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "\n",
    "If Living area is this skewed and Gross Area follows\n",
    "we must test Gross Area'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2028d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['Gross_Area'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Gross_Area'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation\n",
    "df_train['Gross_Area'] = np.log(df_train['Gross_Area'])\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['Gross_Area'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Gross_Area'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['Lot_SQFT'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Lot_SQFT'], plot=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation\n",
    "df_train['Lot_SQFT'] = np.log(df_train['Lot_SQFT'])\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['Lot_SQFT'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['Lot_SQFT'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make dummy variables\n",
    "#Unique Values in Remodel\n",
    "print(df_train['Remodel'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Variables\n",
    "df_train = pd.get_dummies(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now let's do Data Science\n",
    "\n",
    "My Plan:\n",
    "I plan to use several methods\n",
    "Linear Regression (different forms)\n",
    "Boosted Gradient\n",
    "BayesianRidge\n",
    "\n",
    "then simply stack these methods\n",
    "\n",
    "'''\n",
    "#Split dataframe into train and test\n",
    "train, test = train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.Total_Value.values\n",
    "y_test = test.Total_Value.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['Total_Value'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f272e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS libraries\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Before building models I KNOW \n",
    "this dataset contains outliers\n",
    "I know my linear regression models\n",
    "are sensitive to outliers\n",
    "so we can make them more robust using\n",
    "sklearn (Robustscaler()) GBoost(loss = huber)\n",
    "'''\n",
    "#Linear Regression\n",
    "lin_reg = make_pipeline(RobustScaler(), LinearRegression())\n",
    "\n",
    "#Bayesian Ridge\n",
    "BayRidge = make_pipeline(RobustScaler(), BayesianRidge(n_iter = 30))\n",
    "\n",
    "#Lasso Regression\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "#Elastic Net Regression\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "#Kernal Ridge Regression\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "#Gradient Boosting\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb02084",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(lin_reg)\n",
    "print(\"\\nLinear Regression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba19851",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(BayRidge)\n",
    "print(\"\\nBayesianRidge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ddd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3587b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73395d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ce569",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple stacking class and function\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee109ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could add many more models but I wanted to have my top 3\n",
    "averaged_models = AveragingModels(models = (model_xgb, GBoost, KRR))\n",
    "\n",
    "#Use function to find rsme\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning\n",
    "So stacking our 3 best performing models\n",
    "did result in a better score but only slightly\n",
    "\n",
    "we went from a rough 10.34% average error to\n",
    "a 9.7% on a 100k house that saves us $500 per house\n",
    "\n",
    "let's sell 2 a month, 12k a year is a nice bonus \n",
    "just from stacking these. With different boosting\n",
    "params we may get it under 9%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit models from average\n",
    "averaged_models.fit(train, y_train)\n",
    "stacked_train_pred = averaged_models.predict(train)\n",
    "\n",
    "#Convert to base from logs\n",
    "stacked_pred = np.expm1(averaged_models.predict(test.values))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a30358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset setup to have predicted values in df\n",
    "predicted_df = test.copy(deep=True)\n",
    "predicted_df['predicted_Total_Value'] = stacked_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e66325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Show Data frame with our predictions\n",
    "print(predicted_df)\n",
    "\n",
    "#Prepare dataframe in csv for model submission\n",
    "predicted_df.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's build a first pass simple linear\n",
    "regression like what we built in Excel Module 2\n",
    "\n",
    "Let's use the log values to smooth it out \n",
    "'''\n",
    "#show Linear Regression like the Excel First Pass\n",
    "regr = LinearRegression()\n",
    "regr.fit(train, y_train)\n",
    "\n",
    "#Linear Reg prediction\n",
    "'''Logs removed use np.expm1'''\n",
    "#y_predict = np.expm1(regr.predict(test.values))\n",
    "#y_test = np.expm1(test.Total_Value.values)\n",
    "y_predict = regr.predict(test.values)\n",
    "\n",
    "score = rmsle_cv(regr)\n",
    "\n",
    "#Coefficients\n",
    "#print(\"Coefficients: \\n\", regr.coef_)\n",
    "cdf = pd.DataFrame(regr.coef_, train.columns, columns = [\"Coefficients\"])\n",
    "\n",
    "print(cdf)\n",
    "\n",
    "#r2 and mean squared values\n",
    "r_squared = r2_score(y_test,y_predict)\n",
    "mean_square = mean_squared_error((y_test),(y_predict))\n",
    "root_mean_square = mean_squared_error((y_test),(y_predict) ,squared = False)\n",
    "\n",
    "print(\"\\nLinear Regression Mean Error score: {:.4f}\\n\".format(mean_square))\n",
    "print(\"\\nLinear Regression Root Mean Error score: {:.4f}\\n\".format(root_mean_square))\n",
    "print(\"\\nLinear Regression  R squared score: {:.4f}\\n\".format(r_squared))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_prediction_df = pd.DataFrame(np.hstack((y_test)), columns= [\"test\"])\n",
    "\n",
    "test_to_prediction_df['prediction'] = np.hstack((y_predict))\n",
    "\n",
    "test_to_prediction_df['error_value'] = test_to_prediction_df['test']-test_to_prediction_df['prediction']\n",
    "\n",
    "print(\"\\nSum of errors by subtracting prediction from test values: {:.4f}\\n\".format(sum(test_to_prediction_df['error_value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430c128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
